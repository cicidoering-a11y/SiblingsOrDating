{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yuDdYuxDBjv"
      },
      "source": [
        "# Are They Siblings or Dating?\n",
        "## A Deep Learning Image Classification Project\n",
        "### By Andy Phan, Sanjay Karunamoorthy, Kevin Arleen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XkcwK3EUS6H",
        "outputId": "a2234465-7776-47b6-a655-b1ef4f7605e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting prawNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "     -------------------------------------- 189.3/189.3 kB 3.9 MB/s eta 0:00:00\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
            "     ---------------------------------------- 82.6/82.6 kB 4.8 MB/s eta 0:00:00\n",
            "Collecting prawcore<3,>=2.4\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Collecting update_checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.28.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.11.12)\n",
            "Installing collected packages: websocket-client, update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0 websocket-client-1.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UbUH1U1PjlI",
        "outputId": "cf3c0604-6661-4249-fa15-8c2f0cca6ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp310-cp310-win_amd64.whl (331.7 MB)\n",
            "     -------------------------------------- 331.7/331.7 MB 5.5 MB/s eta 0:00:00\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (25.0)\n",
            "Collecting keras>=3.10.0\n",
            "  Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
            "     ---------------------------------------- 1.5/1.5 MB 13.4 MB/s eta 0:00:00\n",
            "Collecting absl-py>=1.0.0\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "     -------------------------------------- 135.8/135.8 kB 4.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: setuptools in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (65.5.0)\n",
            "Collecting google_pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting protobuf>=5.28.0\n",
            "  Downloading protobuf-6.33.2-cp310-abi3-win_amd64.whl (436 kB)\n",
            "     ------------------------------------- 436.9/436.9 kB 26.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (4.15.0)\n",
            "Collecting numpy>=1.26.0\n",
            "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
            "     --------------------------------------- 12.9/12.9 MB 12.8 MB/s eta 0:00:00\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-2.0.1-cp310-cp310-win_amd64.whl (60 kB)\n",
            "     ---------------------------------------- 60.4/60.4 kB 3.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (2.28.1)\n",
            "Collecting tensorboard~=2.20.0\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "     ---------------------------------------- 5.5/5.5 MB 17.7 MB/s eta 0:00:00\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.76.0-cp310-cp310-win_amd64.whl (4.7 MB)\n",
            "     ---------------------------------------- 4.7/4.7 MB 14.3 MB/s eta 0:00:00\n",
            "Collecting ml_dtypes<1.0.0,>=0.5.1\n",
            "  Downloading ml_dtypes-0.5.4-cp310-cp310-win_amd64.whl (210 kB)\n",
            "     ------------------------------------- 210.7/210.7 kB 12.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (1.16.0)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting h5py>=3.11.0\n",
            "  Downloading h5py-3.15.1-cp310-cp310-win_amd64.whl (2.9 MB)\n",
            "     ---------------------------------------- 2.9/2.9 MB 15.3 MB/s eta 0:00:00\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "     --------------------------------------- 26.4/26.4 MB 18.2 MB/s eta 0:00:00\n",
            "Collecting opt_einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "     ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
            "  Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting flatbuffers>=24.3.25\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Collecting optree\n",
            "  Downloading optree-0.18.0-cp310-cp310-win_amd64.whl (302 kB)\n",
            "     -------------------------------------- 302.7/302.7 kB 6.2 MB/s eta 0:00:00\n",
            "Collecting rich\n",
            "  Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "     -------------------------------------- 243.4/243.4 kB 7.5 MB/s eta 0:00:00\n",
            "Collecting namex\n",
            "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.13)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
            "     -------------------------------------- 225.0/225.0 kB 6.9 MB/s eta 0:00:00\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
            "     -------------------------------------- 107.7/107.7 kB 3.0 MB/s eta 0:00:00\n",
            "Collecting pillow\n",
            "  Downloading pillow-12.0.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
            "     ---------------------------------------- 7.0/7.0 MB 21.3 MB/s eta 0:00:00\n",
            "Collecting markupsafe>=2.1.1\n",
            "  Downloading markupsafe-3.0.3-cp310-cp310-win_amd64.whl (15 kB)\n",
            "Collecting markdown-it-py>=2.2.0\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "     ---------------------------------------- 87.3/87.3 kB ? eta 0:00:00\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, wrapt, termcolor, tensorboard-data-server, protobuf, pillow, optree, opt_einsum, numpy, mdurl, markupsafe, markdown, grpcio, google_pasta, gast, astunparse, absl-py, werkzeug, ml_dtypes, markdown-it-py, h5py, tensorboard, rich, keras, tensorflow\n",
            "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 markupsafe-3.0.3 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 numpy-2.2.6 opt_einsum-3.4.0 optree-0.18.0 pillow-12.0.0 protobuf-6.33.2 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 werkzeug-3.1.4 wrapt-2.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
            "     ---------------------------------------- 8.9/8.9 MB 9.3 MB/s eta 0:00:00\n",
            "Collecting joblib>=1.2.0\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "     -------------------------------------- 308.4/308.4 kB 2.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
            "Collecting scipy>=1.8.0\n",
            "  Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
            "     --------------------------------------- 41.3/41.3 MB 10.1 MB/s eta 0:00:00\n",
            "Collecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.7-cp310-cp310-win_amd64.whl (8.1 MB)\n",
            "     ---------------------------------------- 8.1/8.1 MB 10.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Collecting pyparsing>=3\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "     -------------------------------------- 113.9/113.9 kB 3.2 MB/s eta 0:00:00\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.61.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
            "     ---------------------------------------- 1.5/1.5 MB 16.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from matplotlib) (12.0.0)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
            "     -------------------------------------- 221.2/221.2 kB 6.6 MB/s eta 0:00:00\n",
            "Collecting kiwisolver>=1.3.1\n",
            "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
            "     ---------------------------------------- 73.7/73.7 kB 4.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.0 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (2.20.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (2.2.6)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (65.5.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (2.28.1)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (3.12.0)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow) (6.33.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: optree in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: rich in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: pillow in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\cicid\\appdata\\local\\r-miniconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Uv0ALuxDVleF"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import sklearn\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MK69kZJ2WoSL"
      },
      "outputs": [],
      "source": [
        "reddit = praw.Reddit(\n",
        "    client_id=\"SZ6ae6M47WrdQ37nACw10Q\",\n",
        "    client_secret=\"XqzJnY1D3g2_uy1Mlu-Su65lsa2fAg\",\n",
        "    user_agent=\"SiblingsOrDatingScraper\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8tv8YSr0XCNU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(reddit.read_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GKPrbMRKXCfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We fixed the No Answer issue...hopefully. PLEASE READ POSTERS\n",
            "Siblings or dating\n",
            "SOD?\n",
            "Siblings or Dating?\n",
            "Siblings or Dating?\n",
            "siblings or dating?\n",
            "Siblings or dating?\n",
            "Siblings or dating\n",
            "Siblings or dating?\n",
            "SOD\n"
          ]
        }
      ],
      "source": [
        "subreddit = reddit.subreddit(\"siblingsordating\")\n",
        "for submission in subreddit.hot(limit=10):\n",
        "    print(submission.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SAMzPbjUXOg4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped post 0: No author label found\n",
            "Saved: images\\dating\\1.jpg | Label: dating\n",
            "Skipped post 2: No author label found\n",
            "Saved: images\\siblings\\3.jpg | Label: siblings\n",
            "Saved: images\\dating\\4.jpg | Label: dating\n",
            "Skipped post 5: No author label found\n",
            "Saved: images\\dating\\6.jpg | Label: dating\n",
            "Saved: images\\dating\\7.jpg | Label: dating\n",
            "Skipped post 8: No author label found\n",
            "Saved: images\\siblings\\9.jpg | Label: siblings\n",
            "Skipped post 12: No author label found\n",
            "Saved: images\\dating\\13.jpg | Label: dating\n",
            "Saved: images\\siblings\\15.jpg | Label: siblings\n",
            "Saved: images\\dating\\17.jpg | Label: dating\n",
            "Skipped post 18: No author label found\n",
            "Saved: images\\dating\\19.jpg | Label: dating\n",
            "Saved: images\\siblings\\20.jpg | Label: siblings\n",
            "Saved: images\\dating\\21.jpg | Label: dating\n",
            "Saved: images\\siblings\\22.jpg | Label: siblings\n",
            "Saved: images\\dating\\23.jpg | Label: dating\n",
            "Saved: images\\dating\\24.jpg | Label: dating\n",
            "Saved: images\\dating\\25.jpg | Label: dating\n",
            "Saved: images\\dating\\26.jpg | Label: dating\n",
            "Saved: images\\dating\\27.jpg | Label: dating\n",
            "Saved: images\\dating\\28.jpg | Label: dating\n",
            "Skipped post 29: No author label found\n",
            "Skipped post 30: No author label found\n",
            "Saved: images\\siblings\\31.jpg | Label: siblings\n",
            "Saved: images\\dating\\32.jpg | Label: dating\n",
            "Saved: images\\dating\\34.jpg | Label: dating\n",
            "Saved: images\\dating\\35.jpg | Label: dating\n",
            "Saved: images\\dating\\36.jpg | Label: dating\n",
            "Saved: images\\siblings\\38.jpg | Label: siblings\n",
            "Saved: images\\siblings\\39.jpg | Label: siblings\n",
            "Saved: images\\dating\\40.jpg | Label: dating\n",
            "Saved: images\\siblings\\41.jpg | Label: siblings\n",
            "Saved: images\\siblings\\42.jpg | Label: siblings\n",
            "Skipped post 43: No author label found\n",
            "Saved: images\\dating\\44.jpg | Label: dating\n",
            "Saved: images\\dating\\45.jpg | Label: dating\n",
            "Saved: images\\dating\\46.jpg | Label: dating\n",
            "Saved: images\\siblings\\47.jpg | Label: siblings\n",
            "Saved: images\\dating\\48.jpg | Label: dating\n",
            "Saved: images\\siblings\\50.jpg | Label: siblings\n",
            "Saved: images\\dating\\51.jpg | Label: dating\n",
            "Saved: images\\dating\\52.jpg | Label: dating\n",
            "Saved: images\\dating\\53.jpg | Label: dating\n",
            "Saved: images\\dating\\54.jpg | Label: dating\n",
            "Saved: images\\dating\\55.jpg | Label: dating\n",
            "Saved: images\\dating\\56.jpg | Label: dating\n",
            "Saved: images\\dating\\57.jpg | Label: dating\n",
            "Skipped post 58: No author label found\n",
            "Saved: images\\siblings\\59.jpg | Label: siblings\n",
            "Skipped post 60: No author label found\n",
            "Saved: images\\dating\\61.jpg | Label: dating\n",
            "Saved: images\\dating\\62.jpg | Label: dating\n",
            "Saved: images\\dating\\63.jpg | Label: dating\n",
            "Skipped post 64: No author label found\n",
            "Saved: images\\dating\\65.jpg | Label: dating\n",
            "Saved: images\\siblings\\66.jpg | Label: siblings\n",
            "Saved: images\\dating\\67.jpg | Label: dating\n",
            "Skipped post 68: No author label found\n",
            "Saved: images\\dating\\69.jpg | Label: dating\n",
            "Saved: images\\dating\\70.jpg | Label: dating\n",
            "Saved: images\\dating\\71.jpg | Label: dating\n",
            "Saved: images\\dating\\72.jpg | Label: dating\n",
            "Saved: images\\dating\\73.jpg | Label: dating\n",
            "Saved: images\\dating\\74.jpg | Label: dating\n",
            "Saved: images\\dating\\75.jpg | Label: dating\n",
            "Saved: images\\siblings\\76.jpg | Label: siblings\n",
            "Skipped post 77: No author label found\n",
            "Skipped post 78: No author label found\n",
            "Saved: images\\dating\\79.jpg | Label: dating\n",
            "Saved: images\\dating\\80.jpg | Label: dating\n",
            "Saved: images\\siblings\\81.jpg | Label: siblings\n",
            "Saved: images\\siblings\\83.jpg | Label: siblings\n",
            "Saved: images\\siblings\\84.jpg | Label: siblings\n",
            "Saved: images\\dating\\85.jpg | Label: dating\n",
            "Saved: images\\dating\\86.jpg | Label: dating\n",
            "Skipped post 87: No author label found\n",
            "Saved: images\\siblings\\88.jpg | Label: siblings\n",
            "Saved: images\\dating\\89.jpg | Label: dating\n",
            "Saved: images\\dating\\90.jpg | Label: dating\n",
            "Saved: images\\dating\\91.jpg | Label: dating\n",
            "Saved: images\\dating\\92.jpg | Label: dating\n",
            "Saved: images\\dating\\93.jpg | Label: dating\n",
            "Saved: images\\dating\\94.jpg | Label: dating\n",
            "Skipped post 95: No author label found\n",
            "Saved: images\\siblings\\96.jpg | Label: siblings\n",
            "Saved: images\\siblings\\97.jpg | Label: siblings\n",
            "Saved: images\\siblings\\98.jpg | Label: siblings\n",
            "Skipped post 99: No author label found\n",
            "Saved: images\\dating\\100.jpg | Label: dating\n",
            "Saved: images\\dating\\101.jpg | Label: dating\n",
            "Saved: images\\siblings\\102.jpg | Label: siblings\n",
            "Saved: images\\siblings\\103.jpg | Label: siblings\n",
            "Skipped post 104: No author label found\n",
            "Saved: images\\dating\\105.jpg | Label: dating\n",
            "Skipped post 107: No author label found\n",
            "Saved: images\\dating\\108.jpg | Label: dating\n",
            "Saved: images\\dating\\109.jpg | Label: dating\n",
            "Saved: images\\dating\\110.jpg | Label: dating\n",
            "Saved: images\\siblings\\111.jpg | Label: siblings\n",
            "Saved: images\\dating\\112.jpg | Label: dating\n",
            "Saved: images\\dating\\113.jpg | Label: dating\n",
            "Saved: images\\dating\\114.jpg | Label: dating\n",
            "Saved: images\\dating\\115.jpg | Label: dating\n",
            "Saved: images\\dating\\116.jpg | Label: dating\n",
            "Saved: images\\dating\\117.jpg | Label: dating\n",
            "Saved: images\\dating\\118.jpg | Label: dating\n",
            "Skipped post 119: No author label found\n",
            "Saved: images\\siblings\\120.jpg | Label: siblings\n",
            "Saved: images\\dating\\121.jpg | Label: dating\n",
            "Saved: images\\dating\\123.jpg | Label: dating\n",
            "Skipped post 124: No author label found\n",
            "Saved: images\\siblings\\126.jpg | Label: siblings\n",
            "Saved: images\\dating\\127.jpg | Label: dating\n",
            "Saved: images\\dating\\128.jpg | Label: dating\n",
            "Saved: images\\siblings\\129.jpg | Label: siblings\n",
            "Skipped post 130: No author label found\n",
            "Skipped post 131: No author label found\n",
            "Saved: images\\dating\\132.jpg | Label: dating\n",
            "Skipped post 133: No author label found\n",
            "Saved: images\\dating\\134.jpg | Label: dating\n",
            "Saved: images\\dating\\135.jpg | Label: dating\n",
            "Skipped post 136: No author label found\n",
            "Saved: images\\dating\\137.jpg | Label: dating\n",
            "Saved: images\\siblings\\139.jpg | Label: siblings\n",
            "Saved: images\\siblings\\140.jpg | Label: siblings\n",
            "Saved: images\\dating\\141.jpg | Label: dating\n",
            "Saved: images\\dating\\142.jpg | Label: dating\n",
            "Saved: images\\dating\\143.jpg | Label: dating\n",
            "Saved: images\\dating\\144.jpg | Label: dating\n",
            "Saved: images\\dating\\145.jpg | Label: dating\n",
            "Saved: images\\dating\\146.jpg | Label: dating\n",
            "Saved: images\\siblings\\148.jpg | Label: siblings\n",
            "Saved: images\\dating\\150.jpg | Label: dating\n",
            "Saved: images\\siblings\\151.jpg | Label: siblings\n",
            "Saved: images\\siblings\\152.jpg | Label: siblings\n",
            "Skipped post 153: No author label found\n",
            "Saved: images\\dating\\154.jpg | Label: dating\n",
            "Saved: images\\siblings\\155.jpg | Label: siblings\n",
            "Saved: images\\siblings\\156.jpg | Label: siblings\n",
            "Saved: images\\siblings\\157.jpg | Label: siblings\n",
            "Skipped post 158: No author label found\n",
            "Saved: images\\siblings\\159.jpg | Label: siblings\n",
            "Saved: images\\siblings\\160.jpg | Label: siblings\n",
            "Skipped post 162: No author label found\n",
            "Saved: images\\siblings\\163.jpg | Label: siblings\n",
            "Skipped post 164: No author label found\n",
            "Saved: images\\siblings\\165.jpg | Label: siblings\n",
            "Skipped post 166: No author label found\n",
            "Saved: images\\dating\\167.jpg | Label: dating\n",
            "Saved: images\\dating\\169.jpg | Label: dating\n",
            "Saved: images\\siblings\\170.jpg | Label: siblings\n",
            "Saved: images\\dating\\171.jpg | Label: dating\n",
            "Saved: images\\dating\\172.jpg | Label: dating\n",
            "Skipped post 173: No author label found\n",
            "Saved: images\\dating\\174.jpg | Label: dating\n",
            "Skipped post 175: No author label found\n",
            "Skipped post 176: No author label found\n",
            "Saved: images\\dating\\177.jpg | Label: dating\n",
            "Saved: images\\dating\\178.jpg | Label: dating\n",
            "Skipped post 179: No author label found\n",
            "Skipped post 180: No author label found\n",
            "Saved: images\\siblings\\181.jpg | Label: siblings\n",
            "Saved: images\\dating\\182.jpg | Label: dating\n",
            "Skipped post 183: No author label found\n",
            "Saved: images\\dating\\184.jpg | Label: dating\n",
            "Saved: images\\dating\\186.jpg | Label: dating\n",
            "Saved: images\\dating\\188.jpg | Label: dating\n",
            "Skipped post 189: No author label found\n",
            "Saved: images\\dating\\190.jpg | Label: dating\n",
            "Saved: images\\dating\\191.jpg | Label: dating\n",
            "Saved: images\\dating\\192.jpg | Label: dating\n",
            "Skipped post 193: No author label found\n",
            "Saved: images\\dating\\194.jpg | Label: dating\n",
            "Saved: images\\siblings\\195.jpg | Label: siblings\n",
            "Skipped post 196: No author label found\n",
            "Skipped post 198: No author label found\n",
            "Skipped post 199: No author label found\n",
            "Skipped post 200: No author label found\n",
            "Saved: images\\dating\\201.jpg | Label: dating\n",
            "Skipped post 202: No author label found\n",
            "Skipped post 203: No author label found\n",
            "Saved: images\\dating\\204.jpg | Label: dating\n",
            "Saved: images\\dating\\205.jpg | Label: dating\n",
            "Saved: images\\dating\\206.jpg | Label: dating\n",
            "Skipped post 207: No author label found\n",
            "Saved: images\\dating\\208.jpg | Label: dating\n",
            "Saved: images\\dating\\209.jpg | Label: dating\n",
            "Saved: images\\dating\\210.jpg | Label: dating\n",
            "Skipped post 211: No author label found\n",
            "Skipped post 212: No author label found\n",
            "Skipped post 213: No author label found\n",
            "Saved: images\\siblings\\214.jpg | Label: siblings\n",
            "Saved: images\\dating\\215.jpg | Label: dating\n",
            "Saved: images\\dating\\216.jpg | Label: dating\n",
            "Saved: images\\siblings\\218.jpg | Label: siblings\n",
            "Saved: images\\dating\\220.jpg | Label: dating\n",
            "Saved: images\\siblings\\221.jpg | Label: siblings\n",
            "Saved: images\\dating\\222.jpg | Label: dating\n",
            "Skipped post 223: No author label found\n",
            "Skipped post 224: No author label found\n",
            "Skipped post 226: No author label found\n",
            "Saved: images\\dating\\227.jpg | Label: dating\n",
            "Skipped post 228: No author label found\n",
            "Skipped post 230: No author label found\n",
            "Saved: images\\dating\\231.jpg | Label: dating\n",
            "Skipped post 232: No author label found\n",
            "Saved: images\\siblings\\233.jpg | Label: siblings\n",
            "Skipped post 234: No author label found\n",
            "Saved: images\\dating\\235.jpg | Label: dating\n",
            "Saved: images\\siblings\\236.jpg | Label: siblings\n",
            "Saved: images\\dating\\238.jpg | Label: dating\n",
            "Saved: images\\dating\\240.jpg | Label: dating\n",
            "Skipped post 241: No author label found\n",
            "Saved: images\\siblings\\242.jpg | Label: siblings\n",
            "Saved: images\\dating\\243.jpg | Label: dating\n",
            "Skipped post 244: No author label found\n",
            "Saved: images\\siblings\\245.jpg | Label: siblings\n",
            "Saved: images\\dating\\246.jpg | Label: dating\n",
            "Saved: images\\siblings\\247.jpg | Label: siblings\n",
            "Skipped post 248: No author label found\n"
          ]
        }
      ],
      "source": [
        "base_dir = \"images\"\n",
        "os.makedirs(os.path.join(base_dir, \"siblings\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_dir, \"dating\"), exist_ok=True)\n",
        "\n",
        "for i, post in enumerate(subreddit.top(limit=1000)):\n",
        "    if post.url.endswith(('.jpg', '.png', '.jpeg', '.webp')):\n",
        "        post.comments.replace_more(limit=0)\n",
        "        author_label = None\n",
        "\n",
        "        def extract_label(text):\n",
        "            text = text.lower()\n",
        "            if 'sib' in text and 'dating' not in text:\n",
        "                return 'siblings'\n",
        "            elif ('dating' in text or 'couple' in text) and 'sib' not in text:\n",
        "                return 'dating'\n",
        "            return None\n",
        "\n",
        "        # Step 1: Check top-level comments from the author\n",
        "        for comment in post.comments:\n",
        "            if comment.author and post.author and comment.author.name == post.author.name:\n",
        "                author_label = extract_label(comment.body)\n",
        "                if author_label:\n",
        "                    break\n",
        "\n",
        "        # Step 2: Check replies to others if no label found yet\n",
        "        if not author_label:\n",
        "            for comment in post.comments:\n",
        "                for reply in comment.replies:\n",
        "                    if reply.author and post.author and reply.author.name == post.author.name:\n",
        "                        author_label = extract_label(reply.body)\n",
        "                        if author_label:\n",
        "                            break\n",
        "                if author_label:\n",
        "                    break\n",
        "\n",
        "        # Save image to correct subfolder\n",
        "        if author_label:\n",
        "            try:\n",
        "                label_dir = os.path.join(base_dir, author_label)\n",
        "                filename = os.path.join(label_dir, f\"{i}.jpg\")\n",
        "                image_data = requests.get(post.url).content\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(image_data)\n",
        "                print(f\"Saved: {filename} | Label: {author_label}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download image: {e}\")\n",
        "        else:\n",
        "            print(f\"Skipped post {i}: No author label found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4Ly6Ps76CqVi"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m      5\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/images\"\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "uploaded = files.upload()\n",
        "with zipfile.ZipFile(\"images.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"images\")\n",
        "empty_files = []\n",
        "\n",
        "for root, _, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        path = os.path.join(root, file)\n",
        "        if os.path.getsize(path) == 0:\n",
        "            empty_files.append(path)\n",
        "\n",
        "if empty_files:\n",
        "    print(\"Empty image files found:\")\n",
        "    for path in empty_files:\n",
        "        print(path)\n",
        "else:\n",
        "    print(\"No empty files found.\")\n",
        "for path in empty_files:\n",
        "    os.remove(path)\n",
        "    print(f\"Deleted: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8IiB6lm2w1-"
      },
      "outputs": [],
      "source": [
        "valid_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
        "removed = 0\n",
        "\n",
        "for root, _, files in os.walk(\"images\"):\n",
        "    for file in files:\n",
        "        path = os.path.join(root, file)\n",
        "        ext = os.path.splitext(file)[1].lower()\n",
        "        if ext not in valid_exts or os.path.getsize(path) == 0:\n",
        "            os.remove(path)\n",
        "            print(f\"Deleted: {path}\")\n",
        "            removed += 1\n",
        "\n",
        "print(f\"Cleanup complete. Removed {removed} invalid or empty files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7a6w4Ij24kK"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "for root, _, files in os.walk(\"images\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".webp\"):\n",
        "            path = os.path.join(root, file)\n",
        "            try:\n",
        "                img = Image.open(path).convert(\"RGB\")\n",
        "                new_path = path.replace(\".webp\", \".jpg\")\n",
        "                img.save(new_path, \"JPEG\")\n",
        "                os.remove(path)\n",
        "                print(f\"Converted: {file} â†’ .jpg\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to convert {file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWvLL7_w3PV_",
        "outputId": "6b0699f4-9330-4819-9582-a8b0cfb64d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found 0 bad files.\n"
          ]
        }
      ],
      "source": [
        "bad_files = []\n",
        "\n",
        "for root, _, files in os.walk(\"images\"):\n",
        "    for file in files:\n",
        "        filepath = os.path.join(root, file)\n",
        "        try:\n",
        "            img = tf.io.read_file(filepath)\n",
        "            img = tf.image.decode_image(img, channels=3)\n",
        "        except Exception as e:\n",
        "            print(f\"Corrupted or unreadable file: {filepath} | Error: {e}\")\n",
        "            bad_files.append(filepath)\n",
        "\n",
        "print(f\"\\nFound {len(bad_files)} bad files.\")\n",
        "\n",
        "for path in bad_files:\n",
        "    os.remove(path)\n",
        "    print(f\"Deleted: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCdF0lzN1NOc"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"images\", \"zip\", \"images\")\n",
        "files.download(\"images.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4tQ2StzYoBo",
        "outputId": "bac37fd8-976a-4ab9-a4bb-f9f8d68fd068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 192 files belonging to 2 classes.\n",
            "Using 154 files for training.\n",
            "Found 192 files belonging to 2 classes.\n",
            "Using 38 files for validation.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "img_height, img_width = 224, 224\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split = 0.2,\n",
        "    subset = \"training\",\n",
        "    seed = 42,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split = 0.2,\n",
        "    subset = \"validation\",\n",
        "    seed = 42,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A55_PVbC1Tt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Rescaling\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
        "\n",
        "rescale_layer = Rescaling(1/.255)\n",
        "\n",
        "data_augmentation = Sequential([\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.1),\n",
        "    RandomZoom(0.1)\n",
        "])\n",
        "\n",
        "def preprocess_images(images, labels):\n",
        "    images = rescale_layer(images)\n",
        "    images = data_augmentation(images)\n",
        "    return images, labels\n",
        "\n",
        "train_ds = train_ds.map(preprocess_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# For validation, just rescale (no augmentation):\n",
        "validation_ds = validation_ds.map(lambda x, y: (rescale_layer(x), y),\n",
        "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = train_ds.shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
        "validation_ds = validation_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "HjNNLwXW1PiL",
        "outputId": "fdfd76d9-6045-4b9d-974b-588651c4d6ca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>tensorflow.python.data.ops.prefetch_op._PrefetchDataset</b><br/>def __init__(input_dataset, buffer_size, slack_period=None, name=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/prefetch_op.py</a>A `Dataset` that asynchronously prefetches its input.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 31);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "tensorflow.python.data.ops.prefetch_op._PrefetchDataset"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZdkY_BjlFOZ"
      },
      "outputs": [],
      "source": [
        "class_names = ['Siblings', 'Dating']\n",
        "\n",
        "n_rows = 8\n",
        "n_cols = 4\n",
        "plt.figure(figsize=(n_cols * 3, n_rows * 3))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(n_rows*n_cols):\n",
        "    plt.subplot(n_rows, n_cols, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[labels[i]], fontsize=12)\n",
        "  plt.subplots_adjust(wspace=.2, hspace=.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKxyTte63JV_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8zdSyrt6Cth",
        "outputId": "781abb17-4aa6-4d4d-8aac-1b3ceea8d33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 794ms/step - accuracy: 0.5893 - loss: 1.0829 - val_accuracy: 0.7368 - val_loss: 0.5713 - learning_rate: 1.0000e-04\n",
            "Epoch 2/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 313ms/step - accuracy: 0.6071 - loss: 0.8172 - val_accuracy: 0.7368 - val_loss: 0.5772 - learning_rate: 1.0000e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 317ms/step - accuracy: 0.6388 - loss: 0.8502 - val_accuracy: 0.7105 - val_loss: 0.5874 - learning_rate: 1.0000e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397ms/step - accuracy: 0.5244 - loss: 0.8337 - val_accuracy: 0.7632 - val_loss: 0.5978 - learning_rate: 1.0000e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 319ms/step - accuracy: 0.5579 - loss: 0.8289 - val_accuracy: 0.6842 - val_loss: 0.6105 - learning_rate: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(\n",
        "    \"best_model.keras\",\n",
        "    save_best_only=True,\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"auto\"\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=1\n",
        "    )\n",
        "\n",
        "y_train = np.concatenate([y.numpy() for _, y in train_ds], axis=0)\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "base_model = MobileNetV2(input_shape=(224, 224, 3),\n",
        "                         include_top=False,\n",
        "                         weights='imagenet')\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=5,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint_cb],\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "# myEpochs = 30\n",
        "# myOptimizer = Adam(learning_rate=0.0001)\n",
        "# myLoss = 'binary_crossentropy'\n",
        "# myMetrics = ['accuracy']\n",
        "# model.compile(\n",
        "#     loss=myLoss,\n",
        "#     optimizer=myOptimizer,\n",
        "#     metrics=myMetrics\n",
        "# )\n",
        "# history = model.fit(train_ds,\n",
        "#                     validation_data=validation_ds,\n",
        "#                     epochs=myEpochs,\n",
        "#                     callbacks=[early_stop, reduce_lr],\n",
        "#                     class_weight=class_weights_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ptmKTyPLeNe",
        "outputId": "7df705d3-01c2-4b3f-8d9c-610c67869202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 734ms/step - accuracy: 0.5345 - loss: 0.7342 - val_accuracy: 0.7105 - val_loss: 0.5755 - learning_rate: 1.0000e-05\n",
            "Epoch 2/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 363ms/step - accuracy: 0.5738 - loss: 0.7169 - val_accuracy: 0.7368 - val_loss: 0.5790 - learning_rate: 1.0000e-05\n",
            "Epoch 3/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 348ms/step - accuracy: 0.5104 - loss: 0.7393 - val_accuracy: 0.7368 - val_loss: 0.5820 - learning_rate: 1.0000e-05\n",
            "Epoch 4/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 350ms/step - accuracy: 0.5680 - loss: 0.6899 - val_accuracy: 0.7105 - val_loss: 0.5843 - learning_rate: 1.0000e-05\n",
            "Epoch 5/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.5405 - loss: 0.7041\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 353ms/step - accuracy: 0.5446 - loss: 0.7027 - val_accuracy: 0.7105 - val_loss: 0.5863 - learning_rate: 1.0000e-05\n",
            "Epoch 6/100\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 350ms/step - accuracy: 0.5967 - loss: 0.6841 - val_accuracy: 0.7105 - val_loss: 0.5876 - learning_rate: 5.0000e-06\n"
          ]
        }
      ],
      "source": [
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "  layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=Adam(1e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint_cb],\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "best_model = tf.keras.models.load_model(\"best_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CEHa6TL27X5a"
      },
      "outputs": [],
      "source": [
        "# model.evaluate(validation_ds)\n",
        "# best_model.evaluate(validation_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgWzxpSb-FYE",
        "outputId": "e46fb12f-e2f2-4d0a-8180-1e05d8454265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7692 - loss: 0.5930\n",
            "Best saved model - val_loss: 0.5977979898452759 val_accuracy: 0.7631579041481018\n"
          ]
        }
      ],
      "source": [
        "# !rm -rf images\n",
        "best_val_loss, best_val_acc = best_model.evaluate(validation_ds)\n",
        "print(\"Best saved model - val_loss:\", best_val_loss, \"val_accuracy:\", best_val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
